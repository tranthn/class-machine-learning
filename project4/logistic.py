#!/usr/bin/env python3
import sys
import numpy as np
import pandas as pd
import math

## gradient descent pseudocode
# initialize weights by assigning random values in [-0.01, 0.01]
#   - d = array size, # features
# repeat
#   - from 0...d, set delta weights (weight changes) to all 0's
#   - from 1...N 
#       - set output o of model to 0 as well
#       - inner loop: j = 0...d
#           - set output of model to o = o + w_j * x_tj # go across all dimensions of vector to add up feature x weight
#       - y = sigmoid(o) # pass result through logistic function to predict class
#       - calculate gradient to determine new weight update, using (real value - y)
#   - apply weight update and go onto next iteration
# stop at convergence (which is?)

##################################################

# logistic sigmoid function for k == 2
#
# arguments
#   - x: array of floats
#
# returns
#   - returns array with exponential value applied over
def sigmoid(x):
    return (1 / 1 + np.exp(-x))

# logistic sigmoid function for k > 2
#
# arguments
#   - x: array of floats
#   - y: array of floats
#
# returns
#   - returns array with exponential value applied over
def sigmoid_multi(x, y):
    return (np.exp(x) / np.sum(np.exp(y)))

# gradient descent function
#
# arguments
#   - df: dataframe we're processing
#
# returns
#   - returns coefficients generated by sigmoid function
def gradient_descent_binary(df = None, label = ''):
    d = df.shape[1]
    n = df.shape[0]
    feats = df.columns
    weights = np.random.uniform(-.01, 0.01, d) # default values for weights
    convergence = 10 # pick random number for now to stop at outer loop

    for i in range(convergence):

        # default weights delta array, with 0's
        weights_delta = np.zeros(d)

        # default model output array, with 0's
        for _, x in df.iterrows():
            output = np.zeros(d)

            # set output of model
            for j in range(d):

                # not sure how x_tj factors here [todo]
                # x_tj - the j-th feature of the t-th instance in df
                output[j] = output[j] + weights[j]

            y = sigmoid(output)

            for j in range(d):

                # rv = 1 if x is in class C1
                # rv = 0 if x is in class C2
                rv = 1.0

                # not sure how x_tj factors here [todo]
                # x_tj - the j-th feature of the t-th instance in df
                weights_delta[j] = weights_delta[j] + (rv - y[j])

        for j in range(d):
            learning_rate = 0.5
            weights[j] = weights[j] + learning_rate * weights[j]

    return weights

def gradient_descent_multi(df = None, label = ''):
    d = df.shape[1]
    n = df.shape[0]
    feats = df.columns
    convergence = 10 # pick random number for now to stop at outer loop

    # get classes and the k-value (# class options)
    df_byclass = df.groupby(by = [label], dropna = False)
    classes = list(df_byclass.groups.keys())
    k = len(classes)
    k1 = k - 1 # for convenience

    # set weights 2-d frame, with dimensions k x d (# classes by # features)
    weights = np.random.uniform(-.01, 0.01, (k, d)) # default values for weights

    for i in range(convergence):

        # default weights delta array, with 0's
        weights_delta = np.zeros((k1, d))

        # default model output array, with 0's
        for _, x in df.iterrows():

            output = np.arange(k1)
            for i in range(0, k1):
                output[i] = 0

                # set output of model
                for j in range(d):

                    # not sure how x_tj factors here [todo]
                    # x_tj - the j-th feature of the t-th instance in df
                    output[i] = output[i] + weights[i][j]

            y = np.arange(k1)
            for i in range(0, k1):
                # unclear what 2nd argument output[k] is referring to here... [todo]
                y[i] = sigmoid_multi(output[i], output[i])

            for i in range(0, k1):
                for j in range(d):
                    # real value, aka class probability?
                    rv = 1.0

                    # not sure how x_tj factors here [todo]
                    # x_tj - the j-th feature of the t-th instance in df
                    weights_delta[i][j] = weights_delta[i][j] + (rv - y[i])

        for i in range(0, k1):
            for j in range(d):
                learning_rate = 0.5
                weights[i][j] = weights[i][j] + learning_rate * weights[i][j]

    return weights